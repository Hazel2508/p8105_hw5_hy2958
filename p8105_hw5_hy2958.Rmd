---
title: "p8105_hw5_hy2958"
output: github_document
date: "2025-11-14"
---

```{r}
library(tidyverse)
set.seed(1)

```

```{r}
#problem 1
has_duplicate_birthday <- function(n) {
  birthdays <- sample(1:365, size = n, replace = TRUE)
  any(duplicated(birthdays))
}

birthday_prob_loop <- function(n, n_sim = 10000) {
  result <- logical(n_sim)  ##create all 'false' list
  for (i in 1:n_sim) {
    result[i] <- has_duplicate_birthday(n)
  }
  mean(result)
}


birthday_df <- tibble(
  group_size = 2:50,
  prob_shared = map_dbl(2:50, birthday_prob_loop)
)

#plot
birthday_df %>%
  ggplot(aes(x = group_size, y = prob_shared)) +
  geom_line()+
  labs(
    title = "Birthday Duplication Simulation",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  ) 
```
The plot shows the estimated probability that at least two people in a group share the same birthday changes with the group size.

When the group size is small, the probability is close to 0, as the group size continues to increase, the probability grows quickly and approaches 1.

```{r}
#problem 2
simulate_one <- function(mu, n = 30, sigma = 5, alpha = 0.05) {
  x <- rnorm(n, mean = mu, sd = sigma)
  tibble(
    mu_hat = mean(x),
    p_value = t.test(x, mu = 0) |> broom::tidy() |> pull(p.value)
  )
}

simulate_many <- function(mu, nn=5000, n = 30, sigma = 5) {
  out <- vector("list", 5000) #create space
  for (i in 1:5000) {
    out[[i]] <- simulate_one(mu = mu, n = n, sigma = sigma)
  }
  bind_rows(out) |>
    mutate(mu_true = mu,
           reject = p_value < 0.05)
}

mu_grid <- 0:6
sim_all <- map_dfr(mu_grid, simulate_many, nn = 5000, n = 30, sigma = 5)

power_df <- sim_all %>% 
  group_by(mu_true)  %>% 
  summarise(power = mean(reject), .groups = "drop")

avg_overall <- sim_all  %>% 
  group_by(mu_true)  %>% 
  summarise(avg_mu_hat = mean(mu_hat), .groups = "drop")

avg_rejected <- sim_all  %>% 
  filter(reject)  %>% 
  group_by(mu_true)  %>% 
  summarise(avg_mu_hat_reject = mean(mu_hat), .groups = "drop")

#plots
#effect size and power
p_power <- ggplot(power_df, aes(x = mu_true, y = power)) +
  geom_line() + geom_point() +
  labs(title = "Power vs Effect Size (one-sample t-test)",
       x = expression(true~mu))

p_power

#the average estimate of ðœ‡Ì‚ and the true value of ðœ‡
p_avg_all <- ggplot(avg_overall, aes(x = mu_true, y = avg_mu_hat)) + 
  geom_line() + 
  geom_point() +
  labs(title = "Average of Sample Mean vs True Mean (All Samples)",
       x = expression(true~mu), y = expression(Mean~of~hat(mu)))

p_avg_all

#the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected 

p_avg_reject <- ggplot(avg_rejected, aes(x = mu_true, y = avg_mu_hat_reject)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Average of Sample Mean vs True Mean (Rejected Samples)",
    x = expression(True~mu),
    y = expression(Mean~of~hat(mu))
  ) 

p_avg_reject
```
plot1: The plot shows a positive association between effect size (true Î¼) and power.
As the true mean Î¼ moves farther from 0 (the null value), the probability of rejecting Hâ‚€ increases sharply.
When Î¼ â‰ˆ 0, power is low (near the nominal Î± = 0.05 level).
As Î¼ increases to around 2â€“3, power rises quickly above 0.5, and for Î¼ â‰¥ 4, power approaches 1.

plot2&3: The sample average of mu hat where the null is rejected is not approximately equal to the true value of mu

This happens because of selection bias, when we condition on statistical significance (p < 0.05), we only keep the samples with unusually large sample means (those that happened to deviate most from 0). 


```{r}
#problem 3.1
data = read_csv("./homicide-data.csv") %>% 
  mutate(city_state = str_c(city, ", ", state)) 
  
city_homicide <- data %>% 
group_by(city_state) %>%
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
    ) 

city_homicide

baltimore <- city_homicide %>%
  filter(city_state == "Baltimore, MD") 

baltimore_test <- prop.test(
  x = baltimore$unsolved,     
  n = baltimore$total         
)

baltimore_tidy <- broom::tidy(baltimore_test)%>%
  select(estimate, conf.low, conf.high)

baltimore_tidy
```
The raw data contains 52179 observations and 12 variables, included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.


```{r}
#problem 3.2
city_props <- city_homicide  %>%
  mutate(
    test = purrr::map2(unsolved, total, ~ prop.test(x = .x, n = .y)),
    tidy = purrr::map(test, broom::tidy) 
    ) %>%
  unnest(tidy) %>%      # unfold list-column
  select(city_state, total, unsolved, estimate, conf.low, conf.high) %>%
  arrange(desc(estimate)) #reorder

city_props %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
   labs(
    title = "Proportion of Unsolved Homicides by City (95% CI)",
    x = "City State",
    y = "Estimated proportion unsolved"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

